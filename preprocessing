import tensorflow as tf

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from sklearn.model_selection import train_test_split

import unicodedata
import re
import numpy as np
import os
import io
import time
import jieba

path_to_zip = tf.keras.utils.get_file(
    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',
    extract=True)

path_to_file=os.path.dirname(path_to_zip)+"/zh-eng/cmn.txt"
path_to_dict="/root/userdict.txt"
jieba.load_userdict(path_to_dict)
def unicode_to_ascii(s):
  return ''.join(c for c in unicodedata.normalize('NFD', s)
      if unicodedata.category(c) != 'Mn')
      
def preprocess_sentence(w):
  w = unicode_to_ascii(w.lower().strip())
  # creating a space between a word and the punctuation following it
  # eg: "he is a boy." => "he is a boy ."
  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation
  w = re.sub(r"([?.!,¿])", r" \1 ", w)
  w = re.sub(r'[" "]+', " ", w)
  # replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")
  w = re.sub(r"[^a-zA-Z?.!,¿]+", " ", w)
  w = w.strip()
  # adding a start and an end token to the sentence
  # so that the model know when to start and stop predicting.
  w = '<start> ' + w + ' <end>'
  return w

def preprocess_sentence_zh(w):
  w = '<start>' + w + '<end>'
  return w

def create_dataset(path, num_examples):
	lines = io.open(path, encoding='UTF-8').read().strip().split('\n')
	word_tuple = [[w for w in l.split('\t')]  for l in lines[:num_examples]]
	en_raw, zh_raw,_ = zip(*word_tuple)
	en=[preprocess_sentence(w) for w in en_raw]
	zh=[preprocess_sentence_zh(w) for w in zh_raw]
	return en,zh

en,zh=create_dataset(path_to_file,None)

def tokenize(lang):
  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
  lang_tokenizer.fit_on_texts(lang)
  tensor = lang_tokenizer.texts_to_sequences(lang)
  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')
  return tensor, lang_tokenizer
def tokenize_zh(lang):
	zh_sentences=[" ".join(ll) for ll in[[w for w in list(jieba.cut(l))] for l in lang]]
  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(
      filters=' ')
  lang_tokenizer.fit_on_texts(lang)
  tensor = lang_tokenizer.texts_to_sequences(lang)
  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,
                                                         padding='post')
  return tensor, lang_tokenizer
def tokenize_zh(lang):
	zh_sentences=[" ".join(ll) for ll in[[w for w in list(jieba.cut(l))] for l in lang]]
	lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=' ')
	lang_tokenizer.fit_on_texts(zh_sentences)
	tensor = lang_tokenizer.texts_to_sequences(zh_sentences)
	tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')
	return tensor, lang_tokenizer
	
def load_dataset(path, num_examples=None):
  # creating cleaned input, output pairs
  targ_lang, inp_lang = create_dataset(path, num_examples)
  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)
  target_tensor, targ_lang_tokenizer = tokenize_zh(targ_lang)
  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer
  
#input_tensor, inp_lang_tokenizer = tokenize(en)
def convert(lang, tensor):
  for t in tensor:
    if t!=0:
      print ("%d ----> %s" % (t, lang.index_word[t]))
convert(inp_lang_tokenizer, input_tensor[0])

#zh_words=["".join(w) for w in jieba.cut(l) for l in zh]
'''def get_zh_words():
	zh_words = list(set([w for l in zh for w in list(jieba.cut(l))]))
	zh_words.sort()
	return zh_words
zh_words=get_zh_words()
zh_vocab_size = len(zh_words)
print(zh_vocab_size)'''

def tokenize_zh(lang):
	zh_sentences=[" ".join(ll) for ll in[[w for w in list(jieba.cut(l))] for l in lang]]
	lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=' ')
	lang_tokenizer.fit_on_texts(zh_sentences)
	tensor = lang_tokenizer.texts_to_sequences(zh_sentences)
	tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')
	return tensor, lang_tokenizer

